Title: Secure Your Home IoT with the CryptoCrochetKey

----

Author: Anuradha Reddy

----

Keywords: crochet, machineLearning, criticalMaking

----

Category: lunch

----

Language: python

----

Gallery:

- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a4e5f4a4b39324f304d306532614c556f5f394b7643412e706e67.png
- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f302a6d4459456a4a5355784b31754b5f4a50.jpg
- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f323430302f312a577831336842565a525637616d324a48376e593230512e706e67.png
- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3839302f312a4f4767314f5031354d7871346736495746694f6836412e706e67.png
- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a786a37496c744577557552786a43634d4b634a714f512e6a706567.jpg
- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a6c787061785932527741466777586b49732d474e55512e706e67.png
- >
  68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a58554433446d6c3464546c532d43664b646a53514d774032782e706e67.png

----

Contextone: During my PhD studies, I found myself feeling stuck with industry-focused IoT and AI narratives. They tended to ignore the sociotechnical realities and imaginations of marginalised folks, including women and people of colour. Working now with hacker communities, I am inspired by femhackers’ skills, their modes of hacking, and especially their use of diverse materials and hardware that challenge extant ideologies around who computation is for and who can benefit from it.

----

Contexttwo: I think my recipe’s contribution lies at the intersection of two broad communities of practice. The first is the crochet/knitting communities who are often highly skilled in their craft but struggle to apply computational thinking. The other is technical communities who are deeply competent in their programming knowledge, but struggle to integrate interdisciplinary ways of knowing and doing in their computing practices. Some prefer to call this intersection the 'STEAM' community.

----

Contextthree: Intersectional feminism draws from the knowledge that every individual has intersecting positions of power and privilege. My submission works to show that interdisciplinary making can cut across these positions of privilege i.e., by drawing attention to the reappropriation of dominant technology using techniques and grammars that are local and specific to craft communities.

----

Shortdesctext: In a brave new world of quantum encryption, cryptocurrencies and NFT transactions, the concept of a ‘key’ is evermore abstracted by mathematical calculations when compared to the flat metal items we store in our pockets and purses. While digitally encrypted keys can make transactions relatively secure—like receiving an email, ordering a pizza, or dis/arming your home, they continue to be cryptic devices for those of us who are not already familiar with the technology nor supported by a civic hacker community. Is it possible to make a physical key, unique enough to avoid forgery while being recognisable to a local computer for making safe transactions? Thanks to a handy textile yarn called ‘Crypto’ and some inspirational reading on programmable yarn, it is possible to physically encrypt a key through crochet and then to teach a computer to recognize the crocheted key using Machine Learning. The *CryptoCrochet-Key* is meant to be part ‘hack’ and part ‘pedagogical device’ that defamiliarizes existing computational practices (learning code) from an intersectional feminist lens. It is meant to be inclusive and empowering for those skilled in other ways than traditional programmers and to show how their skills can contribute to a critical discussion of data-driven technologies—i.e. by putting the tech to radically different uses. This 3-part tutorial begins by introducing CryptoCrochet-Key with instructions for making the key and then moves to the Machine Learning part of the project. The final part shows how to run the CryptoCrochet-Key Machine Learning model securely on a Raspberry Pi 4.

----

Bodycont: 

----

Bodylayout: [{"attrs":[],"columns":[{"blocks":[{"content":{"text":"Crochet\/knitting is a handicraft practice with strong associations to feminine, domestic hobbies and leisure. Although anyone who has engaged in the practice knows that it is, in fact, an intellectually demanding activity, one that takes advantage of the mathematical and mechanical properties of yarn and knitted fabric \u2014 anticipating how the material behaves with every stitch. Dr. Elisabetta Matsumoto, a physicist, has even argued that \u201cyarn is programmable\u201d and \u201cknitting is coding\u201d in her (link: https:\/\/www.nytimes.com\/2019\/05\/17\/science\/math-physics-knitting-matsumoto.html?smid=url-share text: New York Times interview) \u2014 a pedagogy that goes beyond coding binaries (ones and zeroes). Additionally, crocheting is an act of distributed cognition \u2014 a complex choreography between reading coded patterns and making corresponding hand movements with more than a single apparatus, sometimes in a social setting if one can afford it.\n\n(callout: Like other handicrafts, crochet does not separate the mind\u2019s intellect from that of the body(ies).)\n\nThis argument is central to feminist, queer, and disability theories and a critique against positivist data-driven paradigms that strive to model reality from disembodied datasets (Thomson, 1997; Skeggs, 1997; Ahmed and Stacey, 2001; D'Ignazio and Klein, 2020). The act of crochet\/knitting is also a labour of love, joy, and gift-giving \u2014 a kind of pushback and resistance to agile and extractive data-driven technology practices (Orton-Johnson, 2012; Hellstrom, 2013). If a community of crochet practitioners\/fiber artists have a seat at the table with software developers and data science practitioners, what might they do (or say)? CryptoCrochet-Key is one possible answer.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a786a37496c744577557552786a43634d4b634a714f512e6a706567.jpg)\n\nThe word \u2018crypto\u2019 in CryptoCrochet has little to do with cryptography. Rather, the name is borrowed from a yarn called (link: https:\/\/www.ausyarnco.com.au\/products\/browse-by-brand\/panda\/crypto-8-ply\/default.aspx text: crypto 8 ply) manufactured by the Panda yarn company (AusYarnCo). This yarn is multicoloured\/variegated, which lends itself to joyfully crafting objects that are \u201c(link: https:\/\/www.hackster.io\/news\/anuradha-reddy-s-cryptocrochet-key-gives-you-a-unique-huggable-security-token-for-home-iot-222b422ed7f0 text: uniquely unique)\u201d. With each multicoloured stitch, it becomes possible to encrypt a discrete code\/pattern in the shape of an object (in our case, a key) that only you have access to, and something only you can teach your computer to recognize. If you lose the key, you can easily make a new one and then teach your computer to recognize the new key and delete data for the old one. What makes the key-making process with variegated yarn especially enjoyable is one\u2019s loss of control over the yarn\u2019s colours, something one can liken to not being fully in control of how water travels when painting in watercolour. In the tutorial, we will learn how even the Machine Learning model has trouble making colour distinctions, something that we can use to our advantage to deliberately toy with the model.\n\n####Key Takeaways from the Tutorial####\n\n1. Inspiration for thinking of other non-traditional pedagogies for learning \u2018how to code\u2019,\n1. The empowerment to experiment with technologies or methods from industry (e.g. Machine Learning) using everyday materials \u2014 layered, thick, colourful, and patterned (like yarn), and\n1. to re-deploy them for ends that were never intended by their creators through feminist, queering practices. \n\n(callout: The CryptoCrochet-Key is just one of many adversarial things one can make with variegated yarns to tactically encounter smart surveillance and computer vision systems. Going forward, it is possible to imagine exploring an entirely new genre of security authentication via knitted fashions (sweaters, suits) and wearables.)\n\n####Part 1: Making the Key####\n\nHere\u2019s what you will need to make the crocheted key. The first step is purchasing variegated\/multicoloured yarn from your local yarn shop (fun!) and a crochet hook to go with it (the recommended size of the hook should be visible on the yarn packaging). I usually recommend a cotton yarn with a hook size of 3.5-4mm. You will also need some cotton wool for stuffing, stitch markers (or hairpins) for marking rounds and a tapestry needle with a wide eye to weave in loose yarn.\n\nBefore starting to crochet, it is useful to know some standard crochet notations that will appear in this tutorial. We will use only basic crochet stitches like the (link: https:\/\/youtu.be\/o-Il8OcNAZA text: chain stitch) (denoted by \u2018ch\u2019), the (link: https:\/\/youtu.be\/AFk-fdAowbY text: slip stitch) (\u2018sl st\u2019), and the (link: https:\/\/youtu.be\/Ik-GSXWoSak text: single crochet stitch) (\u2018sc\u2019), which makes this a simple starter project. If you would like to learn the full range of crochet stitches, I can\u2019t recommend (link: https:\/\/www.craftyminx.com\/2011\/11\/crochet-school-.html\/ text: CraftyMinx\u2019s crochet school) enough. If you are already familiar with the techniques, then the next section should be a breeze. Remember that the key is made in separate parts \u2014 the top doughnut ring, the long leg and two short legs.\n\nYou can find the (link: https:\/\/github.com\/anu1905\/CryptoCrochet-Key\/blob\/main\/Crochet%20key_pattern_print.pdf text: PDF version of the pattern instructions on Github). Alternatively, if you would like to slip away from the computer screen for this part of the tutorial, I compiled the instructions into an (link: https:\/\/github.com\/anu1905\/CryptoCrochet-Key\/commit\/383d41248bace8658df757d3c2af95648a30bc73#diff-1bff99354953e3ff5cbd5006378e7d5158d2e45a828ae26e35b49cb5171bc3aa text: 8-page zine) that you can print and take with you. \n\n####Part 2: Teaching the Machine to Recognize Your Key####\n\nOnce you\u2019ve made your key, you can teach your computer to recognize it. In technical terms, this entails a complex process of combining the **Transfer Learning technique** (unsupervised Machine Learning \u2014 Neural Nets) with an **Object Detection model** (a pretrained Machine Learning model) and **Computer Vision** (CV) that uses a camera to \u2018see\u2019 the object and learn what it looks like. Executing this feat would ideally require a prior understanding of data science and analytics, but luckily for us, there is readily available software that lets us train Machine Learning models without prerequisite programming skills or data science knowledge.We will use one such browser-based software program called (link: https:\/\/teachablemachine.withgoogle.com\/ text: Teachable Machine) for this part of the tutorial.\n\n(callout: Teachable Machine(TM) is a software to train your computer to recognize unique sounds, images, and body poses, allowing you to personalize your interactions with the computer.)\n\nFor example, with TM we can capture a large number of images of the crochet key via the computer\u2019s webcam \u2014 from different angles and under different light conditions \u2014 and use those images as training data for the model. Once trained, the computer can predict with some degree of confidence whether what it sees is your key or some other object.\n\nThis is easy to do. We start an \u2018Image Project\u2019 and create classes for the kinds of objects we want or do not want the machine or computer to recognize.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a4e5f4a4b39324f304d306532614c556f5f394b7643412e706e67.png)\n\nI made three keys, each with their unique variegated patterns. I wanted my computer to be able to recognize them individually. I named the keys \u2018Candy,\u2019 \u2018Blossom\u2019 and \u2018Sunny\u2019 (for my own visual recognition sake) and created separate classes for each of them.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a58554433446d6c3464546c532d43664b646a53514d774032782e706e67.png)\n\nFor each key\u2019s class, I used the \u2018webcam\u2019 option to feed as many images as possible for that key. I also created a fourth class called \u2018Empty\u2019 because I also trained my computer on what it sees when there is no key in view.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3839302f312a4f4767314f5031354d7871346736495746694f6836412e706e67.png)\n\nOnce data is fed, the next step is to \u2018train the model\u2019. This takes some time depending on the number of images, so it is recommended not to switch browser tabs while the training is ongoing. The result of the training can be viewed in the preview window to the right, which also lets you test your trained model.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f323430302f312a577831336842565a525637616d324a48376e593230512e706e67.png)\n\nWhen I presented the reverse side of Candy to the camera (image below), the model was only 78% confident that it was seeing Candy and 22% confident it was seeing Blossom. This confusion occurs because both Candy and Blossom share similar visual features from their source material but are still different enough to be recognized on their own.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a6c787061785932527741466777586b49732d474e55512e706e67.png)\n\nWe can take advantage of this model uncertainty by creating several look-alike objects that are memorable to us (because we make them) but not easily distinguishable to the machine. This is the opposite of the \u201c10000 bowls of plain oatmeal\u201d problem found in procedural generation algorithms, where a machine can generate 10000 bowls of oatmeal with each oat in a different size and orientation. While every oat is mathematically unique to the machine, the human cannot distinguish them nor appreciate their uniqueness. \n\n(callout: The crochet-key flips this problem around by queering the objects and the training data. Using variegated yarn in this way can help to play within the boundaries of the object detection machine learning models.\n)\n\nWhat is also attractive about TM is that the training happens locally in your browser and not elsewhere on the \u2018cloud\u2019 network, which means it is possible to download and run the TM model offline, thus allowing you to secure your project even further. This is possible by exporting your trained model using the TensorFlow framework \u2014 an open-source framework that supports machine learning on edge devices (e.g. on a browser window, on a tiny computer such as Raspberry Pi, or even a microcontroller such as Arduino Nano BLE Sense). \n\nFor the remainder of the tutorial, I will demonstrate how you can create an offline secure application of the cryptocrochet-key TM model on a smaller computer i.e. the Raspberry Pi 4 using TensorFlow.\n\n####Part 3: CryptoCrochet-Key on a Raspberry Pi 4 with TensorFlow####\n\nWe have come a long way! But truth be told, we still have some way to go and this is my least favourite part of the tutorial because it is the most technical and the part I feel least confident about. However, it is a crucial section as it adds another layer of security to the project.\n\nYou will need a Raspberry Pi 4 to go on from here. Sadly, the previous versions of RPi do not have sufficient processing power to run TensorFlow on them. Additionally, you will need a Raspberry Pi Camera Board v2 and a display board so you can see what the RPi 4 is seeing and recognizing. For the display, I used a PiTFT 2.8\" Resistive Touchscreen board that I already had at home, although (link: https:\/\/learn.adafruit.com\/running-tensorflow-lite-on-the-raspberry-pi-4 text: Adafruit) recommends an all-in-one board called BrainCraft HAT that is tailored especially for Machine Learning with RPi 4. I know this is a lot of expensive gadgetry to ask of a beginner or amateur maker, so if you can access a maker\/hackerspace in your local area that allows you to borrow or rent this equipment, I would recommend that instead. There\u2019s also a learning curve to this part of the tutorial but I think pushing through it can help to overcome the fears that some of us hold for typing terminal commands to install libraries and other software dependencies. In other words, if you are new to hardware making\/hacking, consider this a learning experience than an implementation.\n\n(image: 68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f302a6d4459456a4a5355784b31754b5f4a50.jpg)\n\nI followed this (link: https:\/\/learn.adafruit.com\/teachable-machine-raspberry-pi-tensorflow-camera\/getting-started text: Adafruit tutorial by Andrew Reusch) to help me run the CryptoCrochet-Key TM model on Raspberry Pi 4. The tutorial is generally comprehensive, especially the TM part. However, I realised that there is actually a LOT of software setup and installations to be made for the RPi 4. I found help for this in another tutorial, but to save time, I will list out the steps I followed with links to each of them.\n\n#####Steps#####\n1. (link: https:\/\/learn.adafruit.com\/running-tensorflow-lite-on-the-raspberry-pi-4\/initial-setup text: Initial setup on RPi 4).\n2. (link: https:\/\/learn.adafruit.com\/running-tensorflow-lite-on-the-raspberry-pi-4\/display-setup text: Display setup for both BrainCraft and Resistive Touchscreen).\n3. (link: https:\/\/learn.adafruit.com\/running-tensorflow-lite-on-the-raspberry-pi-4\/camera-test text: Camera setup and test).\n4. (link: https:\/\/learn.adafruit.com\/running-tensorflow-lite-on-the-raspberry-pi-4\/tensorflow-lite-2-setup text: Install TensorFlow) (yes, the title says TensorFlow \u2018Lite\u2019 but it actually installs TensorFlow).\n5. (link: https:\/\/learn.adafruit.com\/teachable-machine-raspberry-pi-tensorflow-camera\/create-categories text: Visit TM and create classes) (you already know this \ud83d\ude42).\n6. (link: https:\/\/learn.adafruit.com\/teachable-machine-raspberry-pi-tensorflow-camera\/use-raspberry-pi-camera text: Capturing images via RPi\u2019s camera).\n7. (link: https:\/\/learn.adafruit.com\/teachable-machine-raspberry-pi-tensorflow-camera\/use-raspberry-pi-camera text: Train model).\n8. (link: https:\/\/learn.adafruit.com\/teachable-machine-raspberry-pi-tensorflow-camera\/transferring-to-the-pi text: Export TensorFlow model from TM and transfer onto the RPi 4).\n\nFollowing these steps may require some patience and gut-feeling, especially if you are a newbie to terminal commands. It does work eventually! I even managed to play with the model\u2019s uncertainty e.g. the GIF image above shows the model\u2019s inability to distinguish between Candy and Sunny. In the end, I never deployed this project as a home security system because part of the fun was just seeing if I could and for it to be a learning experience."},"id":"cc70bf2d-459b-4370-b98f-96003f10b831","isHidden":false,"type":"markdown"}],"id":"cf3fece8-6272-4ccc-b995-9f9f31f0e015","width":"1\/1"}],"id":"363fd27a-0adf-44f0-abda-23c918a9e63a"}]

----

Context: true

----

Shortdesc: true

----

Bodycontent: true

----

Layouttog: true

----

Meta-title: 

----

Meta-description: 

----

Meta-canonical-url: 

----

Meta-author: 

----

Meta-image: 

----

Meta-phone-number: 

----

Og-title: 

----

Og-description: 

----

Og-image: 

----

Og-site-name: 

----

Og-url: 

----

Og-audio: 

----

Og-video: 

----

Og-determiner: 

----

Og-type: website

----

Og-type-article-published-time: 2022-06-21 22:00:00

----

Og-type-article-modified-time: 2022-06-21 22:00:00

----

Og-type-article-expiration-time: 2022-06-21 22:00:00

----

Og-type-article-author: 

----

Og-type-article-section: 

----

Og-type-article-tag: 

----

Twitter-title: 

----

Twitter-description: 

----

Twitter-image: 

----

Twitter-card-type: 

----

Twitter-site: 

----

Twitter-creator: 

----

Robots-noindex: default

----

Robots-nofollow: default

----

Robots-noarchive: default

----

Robots-noimageindex: default

----

Robots-nosnippet: default